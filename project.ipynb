{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of *Attention Is All You Need* by Vaswani et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell",
     "convert-module"
    ]
   },
   "source": [
    "Implementation of Transformer model.\n",
    "\n",
    "This module contains classes and functions which implement the main parts of\n",
    "the Transformer model, as presented in paper Attention Is All You Need\n",
    "by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Embeddings\">Embeddings</h2>\n",
    "\n",
    "Input and outputs sentences are sequences of tokens. Tokens are not necessarily words or characters and they are identified through a specific algorithm, called *tokenizer*. In the case of the original paper, the byte pair encoding algorithm is used (cf. section [Training Data and Batching](#Training_Data_and_Batching) for more details).\n",
    "\n",
    "The resulting set of tokens is called *vocabulary*, its cardinality is $d_\\text{vocabulary}$ and depends on the dataset considered for the task.\n",
    "Special tokens are included to represent the beginning the end of the sentence, the end of the sentence and for padding (i.e. to identify a position in the sentence not occupied by a token with useful meaning).\n",
    "\n",
    "The vocabulary is embedded in a vector space of real numbers $\\mathbb{R}^{d_\\text{model}}$, where $d_\\text{model}$ is a hyperparameter. The embedding is equivalent to a linear layer where the weights are learned during training.\n",
    "This embedding allows the model to learn hidden relations among tokens of the training set, lowering the dimensionality of the vocabulary since $d_\\text{model} < d_\\text{vocabulary}$.\n",
    "\n",
    "The values obtained by the embedding algorithm are scaled by a factor $\\sqrt{d_\\text{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_vocabulary: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_vocabulary = d_vocabulary\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(self.d_vocabulary, self.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
