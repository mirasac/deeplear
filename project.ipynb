{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of *Attention Is All You Need* by Vaswani et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell",
     "convert-module"
    ]
   },
   "source": [
    "Implementation of Transformer model.\n",
    "\n",
    "This module contains classes and functions which implement the main parts of\n",
    "the Transformer model, as presented in paper Attention Is All You Need\n",
    "by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Model_Architecture\">Model Architecture</h2>\n",
    "\n",
    "The Transformer model has an encoder-decoder architecture, where $N$ encoder and decoder layers are stacked using the outputs of the previous layer as inputs.\n",
    "\n",
    "The layers contained in each encoder or decoder layer are addressed as \"sublayers\".\n",
    "\n",
    "The inputs of the first layer are vectors of real numbers with dimension set by the hyperparameter $d_\\text{model}$. Hereafter, vectors in formulae are considered row vectors.\n",
    "The output of the last decoder layer is transformed by a linear layer to obtain vectors of dimension appropriate to the classification task.\n",
    "\n",
    "<img src=\"figures/ModalNet-21.png\" alt=\"The Transformer\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Embeddings_and_Softmax\">Embeddings and Softmax</h3>\n",
    "\n",
    "Input and outputs sentences are sequences of tokens. Tokens are not necessarily words or characters and they are identified through a specific algorithm, called \"tokenizer\". In the case of the original paper, the byte pair encoding algorithm is used (cf. section [Training Data and Batching](#Training_Data_and_Batching) for more details).\n",
    "\n",
    "The resulting set of tokens is called \"vocabulary\", its cardinality is $d_\\text{vocabulary}$ and depends on the dataset considered for the task.\n",
    "Special tokens are included to represent the beginning the end of the sentence, the end of the sentence and for padding (i.e. to identify a position in the sentence not occupied by a token with useful meaning).\n",
    "\n",
    "The vocabulary is embedded in a vector space of real numbers $\\mathbb{R}^{d_\\text{model}}$. The embedding is equivalent to a linear layer where the weights are learned during training.\n",
    "This embedding allows the model to learn hidden relations among tokens of the training set, lowering the dimensionality of the vocabulary since $d_\\text{model} < d_\\text{vocabulary}$.\n",
    "\n",
    "Learned weights are shared among the input and output embedding layers. Moreover, the values obtained by the embedding algorithm are scaled by a factor $\\sqrt{d_\\text{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_vocabulary: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_vocabulary = d_vocabulary\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(self.d_vocabulary, self.d_model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.embedding(input) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights from the embedding layer are shared also with the linear layer positioned before the softmax layer which determines the probabilities of the next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, d_model: int, d_vocabulary: int) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(d_model, d_vocabulary)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.linear(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.softmax(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Positional_Encoding\">Positional Encoding</h3>\n",
    "\n",
    "The meaning of a sentence is determined by the words that are contained and their relative position. Since operations applied to a given sequence are invariant under token permutation, the information on the position of tokens is inserted explicitly in the model.\n",
    "This is achieved by encoding the position of each word in numeric values which are evaluated by analytic formulae. These functions are fixed, i.e. no learning is performed for them, because differences in performance between the two versions are negligible.\n",
    "\n",
    "For each token in a sequence, a vector with same size $d_\\text{model}$ of the embedding is generated by equation\n",
    "\\begin{equation*}\n",
    "    \\mathrm{PE}(\\mathrm{pos}, i) =\n",
    "    \\begin{cases}\n",
    "        \\sin \\bigg( \\frac{\\mathrm{pos}}{10000^{\\frac{i}{d_\\text{model}}}} \\bigg) & \\text{$i$ even} \\\\\n",
    "        \\cos \\bigg( \\frac{\\mathrm{pos}}{10000^{\\frac{i}{d_\\text{model}}}} \\bigg) & \\text{$i$ odd}\n",
    "    \\end{cases}\n",
    "    \\quad ,\n",
    "\\end{equation*}\n",
    "depending on the parity of the element of the vector, with $\\mathrm{pos}$ position of the token in the sequence and $i = 0, \\dots, d_\\text{model} - 1$ index of elements in the vector resulting from the embedding.\n",
    "\n",
    "Trigonometric functions are chosen because they can evaluate positional encodings for sequences longer than the ones encountered during training without additional computations, due to their periodicity.\n",
    "\n",
    "Dropout is applied during training and the dropout probability is stored in the hyperparameter $\\mathrm{dropout}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, d_sequence: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sequence = d_sequence\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zero(d_sequence, d_model)\n",
    "        pos = torch.arange(0, d_sequence, dtype=torch.float).unsqueeze(1)\n",
    "        # Use exp and log to increase performance.\n",
    "        denominator = torch.exp(torch.arange(0, d_model, 2).float() / d_model * math.log(10000))\n",
    "        pe[:, 0::2] = torch.sin(pos / denominator)\n",
    "        pe[:, 1::2] = torch.cos(pos / denominator)\n",
    "        \n",
    "        # Add batch dimension for parallel processing of sequences.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Store positional encoding parameters for future analysis.\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # No need to learn positional encoding parameters.\n",
    "        input = input + (self.pe[:, :input.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Layer_Normalization_and_Residual_Connection\">Layer Normalization and Residual Connection</h3>\n",
    "\n",
    "Layer normalization is the last transformation applied to the output values of each sublayer. Normalization layers help to increase the convergence rate during training and execution of the model.\n",
    "\n",
    "The transformation consists in normalizing the input values using their sample mean and standard deviation, the latter being evaluated with the biased estimator.\n",
    "Learned weights are present to adapt the sample statistics to the dataset. To avoid divergence issues, a constant scalar value $\\mathrm{eps}$ is summed to the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gain = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Dimension is kept to allow broadcasting.\n",
    "        mean = input.mean(dim=-1, keepdim=True)\n",
    "        std = input.std(dim=-1, correction=0, keepdim=True)\n",
    "        return self.gain / (std + self.eps) * (input - mean) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the layer normalization, output values of the sublayer are added to the input values. This sum is called residual connection and helps to reduce the propagation of noise between the connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, eps: float, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.norm = LayerNormalization(eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input, sublayer):\n",
    "        return input + self.dropout(self.norm(sublayer(input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Position-wise_Feed-Forward_Networks\">Position-wise Feed-Forward Networks</h3>\n",
    "\n",
    "A feedforward neural network is applied to data corresponding to each position (i.e. token in the input sequence). The network has 2 layers, with ReLU activation function for each neuron. Dimensions are $d_\\text{model}$ for the output layer and $d_\\text{ff}$ for the hidden layer.\n",
    "\n",
    "The model is\n",
    "\\begin{equation*}\n",
    "    \\mathrm{FFN}(input) = \\max(0, input W_1 + b_1) W_2 + b_2\n",
    "    \\quad ,\n",
    "\\end{equation*}\n",
    "where ReLU function is applied element-wise. Weights are shared among all the positions, but are different between the layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(input))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Attention\">Attention</h3>\n",
    "\n",
    "The \"attention\" is a mapping between vectors corresponding to tokens in a given sequence. Its purpose is to identify the tokens with highest correlation without losing information on the tokens less correlated. These information are then aggregated as output of the function. The attention sublayer implements this mapping.\n",
    "\n",
    "More in detail, the attention function applied to each token is\n",
    "\\begin{equation*}\n",
    "    \\mathrm{Attention}(Q, K, V) = \\mathrm{softmax} \\Bigg( \\frac{Q K^\\intercal}{\\sqrt{d_k}} \\Bigg) V\n",
    "\\end{equation*}\n",
    "where $Q \\in \\mathbb{R}^{d_k}$, $K \\in \\mathbb{R}^{d_k}$ and $V \\in \\mathbb{R}^{d_v}$ are the query, key and value vectors, respectively. Query and key have same dimension to perform their product without introducing additional weights. Values are weighted by the output of a softmax sublayer.\n",
    "\n",
    "The authors call this attention function \"Scaled Dot-Product Attention\", in contrast to the \"additive attention\" which is implemented through a feedforward network with a single hidden layer and the \"dot-product attention\", equivalent to function $\\mathrm{Attention}$ without dividing the argument of the softmax by $\\sqrt{d_k}$. The calculations for dot-product attention are performed more efficiently than for additive attention, but for large values of $d_k$ the former suffers from the vanishing gradients problem due to the saturation of the softmax function. Factor $\\sqrt{d_k}$ in the Scaled Dot-Product Attention addresses this issue.\n",
    "\n",
    "In the Transformer, attention is applied in parallel $h$ times, each application being called \"head\" in a \"multi-head attention\" sublayer. The vectors resulting from the heads are concatenated to obtain a vector which is then multiplied to a weight matrix. The formula presented in the original paper is\n",
    "\\begin{equation*}\n",
    "    \\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head}_1, \\dots, \\mathrm{head}_h) W^O\n",
    "\\end{equation*}\n",
    "where $\\mathrm{head}_i = \\mathrm{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$, index $i = 1, \\dots, h$ identifies the head and the matrices $W_i^Q \\in \\mathbb{R}^{d_\\text{model} \\times d_k}$, $W_i^K \\in \\mathbb{R}^{d_\\text{model} \\times d_k}$, $W_i^V \\in \\mathbb{R}^{d_\\text{model} \\times d_v}$ and $W^O \\in \\mathbb{R}^{h d_v \\times d_\\text{model}}$ are linear projections for query, value, key and output vector, respectively.\n",
    "\n",
    "By chosing $d_k = d_v = \\frac{d_\\text{model}}{h}$ the authors noted that the computational cost is similar to attention with a single head.\n",
    "\n",
    "Moreover, vectors $Q$, $K$ and $V$ are extended to matrices, to parallelize the evaluation on the entire token sequence.\n",
    "This requires that the input vectors to the multi-head attention sublayer are different depending on the position of the sublayer inside the model:\n",
    "\n",
    "- In the encoder layer, query, key and value vectors are the same output vector from the previous sublayer.\n",
    "- In the multi-head attention sublayer which connects a pair of encoder and decoder layers, key and value vectors are the outputs of the encoder layer, while the query is the output of the previous sublayer in the decoder layer. This is similar to sequence-to-sequence models.\n",
    "- In the decoder layer, query, key and value vectors are the same output vector from the previous sublayer, but tokens are correlated only to their or previous positions in the sequence. This condition is achieved by masking the rightmost elements by setting them to $-\\infty$ in the argument of the softmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        # Dimension of embedding is supposed to be divisible by number of heads.\n",
    "        self.d_k = d_model // h\n",
    "        self.d_v = self.d_k\n",
    "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.softmax = Softmax()\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.BoolTensor):\n",
    "        query = self.W_Q(Q)\n",
    "        key = self.W_K(K)\n",
    "        value = self.W_V(V)\n",
    "        \n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.reshape(query.shape[0], query.shape[1], self.h, self.d_k)\n",
    "        key = key.reshape(key.shape[0], key.shape[1], self.h, self.d_k)\n",
    "        value = value.reshape(value.shape[0], value.shape[1], self.h, self.d_v)\n",
    "        attention = torch.matmul(query.transpose(1, 2), key.transpose(1, 2).transpose(2, 3))\n",
    "        # Mask to saturate to zero the softmax function.\n",
    "        if mask is not None:\n",
    "            attention.masked_fill_(mask == 0, -1e15)\n",
    "        attention = self.softmax(attention / math.sqrt(self.d_k), dim=3)\n",
    "        attention = self.dropout(attention)\n",
    "        attention = torch.matmul(attention, value)\n",
    "        \n",
    "        # Concatenate heads.\n",
    "        output = attention.transpose(1, 2)\n",
    "        output.reshape(output.shape[0], output.shape[1], self.h * self.d_k)\n",
    "        return self.W_O(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
