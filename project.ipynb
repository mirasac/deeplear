{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of *Attention Is All You Need* by Vaswani et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove-cell",
     "convert-module"
    ]
   },
   "source": [
    "Implementation of Transformer model.\n",
    "\n",
    "This module contains classes and functions which implement the main parts of\n",
    "the Transformer model, as presented in paper Attention Is All You Need\n",
    "by Vaswani et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Embeddings\">Embeddings</h2>\n",
    "\n",
    "Input and outputs sentences are sequences of tokens. Tokens are not necessarily words or characters and they are identified through a specific algorithm, called *tokenizer*. In the case of the original paper, the byte pair encoding algorithm is used (cf. section [Training Data and Batching](#Training_Data_and_Batching) for more details).\n",
    "\n",
    "The resulting set of tokens is called *vocabulary*, its cardinality is $d_\\text{vocabulary}$ and depends on the dataset considered for the task.\n",
    "Special tokens are included to represent the beginning the end of the sentence, the end of the sentence and for padding (i.e. to identify a position in the sentence not occupied by a token with useful meaning).\n",
    "\n",
    "The vocabulary is embedded in a vector space of real numbers $\\mathbb{R}^{d_\\text{model}}$, where $d_\\text{model}$ is a hyperparameter. The embedding is equivalent to a linear layer where the weights are learned during training.\n",
    "This embedding allows the model to learn hidden relations among tokens of the training set, lowering the dimensionality of the vocabulary since $d_\\text{model} < d_\\text{vocabulary}$.\n",
    "\n",
    "The values obtained by the embedding algorithm are scaled by a factor $\\sqrt{d_\\text{model}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "convert-module"
    ]
   },
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_vocabulary: int, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_vocabulary = d_vocabulary\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(self.d_vocabulary, self.d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Positional_Encoding\">Positional Encoding</h2>\n",
    "\n",
    "The meaning of a sentence is determined by the words that are contained and their relative position. Since operations applied to a given sequence are invariant under token permutation, the information on the position of tokens is inserted explicitly in the model.\n",
    "This is achieved by encoding the position of each word in numeric values which are evaluated by analytic formulae. These functions are fixed, i.e. no learning is performed for them, because differences in performance between the two versions are negligible.\n",
    "\n",
    "For each token in a sequence, a vector with same size $d_\\text{model}$ of the embedding is generated by equations\n",
    "\\begin{align*}\n",
    "    \\mathrm{PE}(\\mathrm{pos}, 2t) & = \\sin \\bigg( \\frac{\\mathrm{pos}}{10000^{\\frac{2t}{d_\\text{model}}}} \\bigg)\n",
    "    \\quad , \\\\\n",
    "    \\mathrm{PE}(\\mathrm{pos}, 2t + 1) & = \\cos \\bigg( \\frac{\\mathrm{pos}}{10000^{\\frac{2t}{d_\\text{model}}}} \\bigg)\n",
    "    \\quad ,\n",
    "\\end{align*}\n",
    "depending on the parity of the element of the vector, with $\\mathrm{pos}$ position of the token in the sequence and $t \\in \\mathbb{N}$ parameter used to identify every element of the vector.\n",
    "\n",
    "Trigonometric functions are chosen because they can evaluate positional encodings for sequences longer than the ones encountered during training without additional computations, due to their periodicity.\n",
    "\n",
    "Dropout is applied during training and the dropout probability is stored in the hyperparameter $\\mathrm{dropout}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, d_sequence: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sequence = d_sequence\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zero(d_sequence, d_model)\n",
    "        pos = torch.arange(0, d_sequence, dtype=torch.float).unsqueeze(1)\n",
    "        denominator = torch.exp(torch.arange(0, d_model, 2).float() / d_model * math.log(10000))\n",
    "        pe[:, 0::2] = torch.sin(pos / denominator)\n",
    "        pe[:, 1::2] = torch.cos(pos / denominator)\n",
    "        \n",
    "        # Add batch dimension for parallel training.\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Store positional encoding values.\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # No learning needed for positional encoding parameters.\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
